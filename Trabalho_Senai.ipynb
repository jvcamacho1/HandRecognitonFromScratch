{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ff55db0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mediapipe'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ImageDataGenerator\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmediapipe\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mmp\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'mediapipe'"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import keras\n",
    "import mediapipe as mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3368f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Using cached mediapipe-0.8.10.1-cp39-cp39-win_amd64.whl (48.7 MB)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from mediapipe) (3.19.4)\n",
      "Collecting opencv-contrib-python\n",
      "  Using cached opencv_contrib_python-4.6.0.66-cp36-abi3-win_amd64.whl (42.5 MB)Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Acesso negado: 'C:\\\\Users\\\\jv_ca\\\\anaconda3\\\\envs\\\\tensorflow\\\\Lib\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from mediapipe) (1.22.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from mediapipe) (3.5.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from mediapipe) (1.2.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib->mediapipe) (9.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib->mediapipe) (4.36.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib->mediapipe) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\jv_ca\\anaconda3\\envs\\tensorflow\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Installing collected packages: opencv-contrib-python, mediapipe\n"
     ]
    }
   ],
   "source": [
    "pip install mediapipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833992e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "path = r'C:\\\\Users\\\\jv_ca\\\\Desktop\\\\TCC2\\\\Hand Gesture Recognition\\\\Datasets\\\\LIBRAS\\\\merged'\n",
    "\n",
    "X = ImageDataGenerator(preprocessing_function=tf.keras.applications.vgg16.preprocess_input, validation_split=0.2)\n",
    "\n",
    "train = X.flow_from_directory(directory=path, \n",
    "                            target_size=(50,50), \n",
    "                            class_mode='categorical',\n",
    "                            batch_size=10,\n",
    "                            shuffle=True, \n",
    "                            subset='training')\n",
    "                            \n",
    "test = X.flow_from_directory(directory=path, \n",
    "                            target_size=(50,50), \n",
    "                            class_mode='categorical',\n",
    "                            batch_size=10,shuffle=True, \n",
    "                            subset='validation')\n",
    "\n",
    "train_step = train.n//train.batch_size\n",
    "test_step = test.n//test.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a0d1e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '1', 1: '2', 2: '4', 3: '5', 4: '7', 5: '9', 6: 'A', 7: 'Adulto', 8: 'Aviao', 9: 'B', 10: 'C', 11: 'D', 12: 'E', 13: 'F', 14: 'G', 15: 'I', 16: 'L', 17: 'M', 18: 'N', 19: 'O', 20: 'P', 21: 'Palavra', 22: 'Pequeno', 23: 'Q', 24: 'R', 25: 'S', 26: 'T', 27: 'U', 28: 'V', 29: 'W', 30: 'X', 31: 'Y'}\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model(r\"C:\\\\Users\\\\jv_ca\\Desktop\\\\TCC2\\\\Hand Gesture Recognition\\\\a3.h5\")\n",
    "d = {}\n",
    "for i in range(32):\n",
    "    d[i] = list(train.class_indices.keys())[i]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6dd6dd92",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m _, frame \u001b[38;5;241m=\u001b[39m cap\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m     12\u001b[0m framergb \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2RGB)\n\u001b[1;32m---> 13\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mhands\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframergb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m hand_landmarks \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mmulti_hand_landmarks\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hand_landmarks:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaasd\\lib\\site-packages\\mediapipe\\python\\solutions\\hands.py:153\u001b[0m, in \u001b[0;36mHands.process\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess\u001b[39m(\u001b[38;5;28mself\u001b[39m, image: np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NamedTuple:\n\u001b[0;32m    133\u001b[0m   \u001b[38;5;124;03m\"\"\"Processes an RGB image and returns the hand landmarks and handedness of each detected hand.\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;124;03m         right hand) of the detected hand.\u001b[39;00m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dsaasd\\lib\\site-packages\\mediapipe\\python\\solution_base.py:364\u001b[0m, in \u001b[0;36mSolutionBase.process\u001b[1;34m(self, input_data)\u001b[0m\n\u001b[0;32m    358\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph\u001b[38;5;241m.\u001b[39madd_packet_to_input_stream(\n\u001b[0;32m    360\u001b[0m         stream\u001b[38;5;241m=\u001b[39mstream_name,\n\u001b[0;32m    361\u001b[0m         packet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_packet(input_stream_type,\n\u001b[0;32m    362\u001b[0m                                  data)\u001b[38;5;241m.\u001b[39mat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_simulated_timestamp))\n\u001b[1;32m--> 364\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_graph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait_until_idle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    365\u001b[0m \u001b[38;5;66;03m# Create a NamedTuple object where the field names are mapping to the graph\u001b[39;00m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;66;03m# output stream names.\u001b[39;00m\n\u001b[0;32m    367\u001b[0m solution_outputs \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mnamedtuple(\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSolutionOutputs\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_stream_type_info\u001b[38;5;241m.\u001b[39mkeys())\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mphands = mp.solutions.hands\n",
    "hands = mphands.Hands()\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "_, frame = cap.read()\n",
    "\n",
    "h, w, c = frame.shape\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    framergb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    result = hands.process(framergb)\n",
    "    hand_landmarks = result.multi_hand_landmarks\n",
    "    if hand_landmarks:\n",
    "        for handLMs in hand_landmarks:\n",
    "            x_max = 0\n",
    "            y_max = 0\n",
    "            x_min = w\n",
    "            y_min = h\n",
    "            for lm in handLMs.landmark:\n",
    "                x, y = int(lm.x * w), int(lm.y * h)\n",
    "                if x > x_max:\n",
    "                    x_max = x\n",
    "                if x < x_min:\n",
    "                    x_min = x\n",
    "                if y > y_max:\n",
    "                    y_max = y\n",
    "                if y < y_min:\n",
    "                    y_min = y\n",
    "            crop = frame[y_min-30:y_max+30,x_min-30:x_max+30].copy()\n",
    "            crop = cv2.resize(crop, (50, 50))\n",
    "            crop = np.reshape(crop,(1,crop.shape[0],crop.shape[1],3))\n",
    "            pred = model.predict(crop)\n",
    "            frame = cv2.rectangle(frame, (x_min-30, y_min-30), (x_max+30, y_max+30), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, d[np.argmax(pred)],(x_min-50, y_min-50), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,255,0), 2)\n",
    "            \n",
    "       \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cec74ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85d1acce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C\n",
      "1\n",
      "B\n",
      "4\n",
      "A\n"
     ]
    }
   ],
   "source": [
    "img = cv2.imread('c.png')\n",
    "img = np.reshape(img,(1,img.shape[0],img.shape[1],3))\n",
    "pred = model.predict(img)\n",
    "print(d[np.argmax(pred)])\n",
    "img = cv2.imread('1.png')\n",
    "img = np.reshape(img,(1,img.shape[0],img.shape[1],3))\n",
    "pred = model.predict(img)\n",
    "print(d[np.argmax(pred)])\n",
    "img = cv2.imread('e.png')\n",
    "img = np.reshape(img,(1,img.shape[0],img.shape[1],3))\n",
    "pred = model.predict(img)\n",
    "print(d[np.argmax(pred)])\n",
    "img = cv2.imread('4.png')\n",
    "img = np.reshape(img,(1,img.shape[0],img.shape[1],3))\n",
    "pred = model.predict(img)\n",
    "print(d[np.argmax(pred)])\n",
    "img = cv2.imread('o.png')\n",
    "img = np.reshape(img,(1,img.shape[0],img.shape[1],3))\n",
    "pred = model.predict(img)\n",
    "print(d[np.argmax(pred)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fad0f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
